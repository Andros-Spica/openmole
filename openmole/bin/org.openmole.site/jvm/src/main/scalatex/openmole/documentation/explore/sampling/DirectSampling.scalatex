@import org.openmole.site.stylesheet._
@import org.openmole.site._
@import org.openmole.site.tools._
@import DocumentationPages._



@def variables = """
val density = Val[Double]
val seed = Val[Int]
val burned = Val[Double]
"""

@def gridSampling = "Grid Sampling"
@def csvSampling = "CSV Sampling"
@def severalInputs = "Exploration of several inputs"
@def advancedSampling = "Advanced sampling"

@b{Design of Experiments} (DoE) is the art of setting up an experimentation. In a model simulation context,
it boils down to declare the inputs under study (most of the time, they're parameters) and the values they will take, for a batch of several simulations, with the idea of revealing a property of the model (e.g. sensitivity).
Even if there are several state-of-the-art DoE methods implemented in OpenMOLE, we recommend to focus on OpenMOLE
new methods:  PSE, and Calibration and Profiles  which have been thought to improve the drawbacks of the classical methods.

@br
Your model inputs can be sampled in the traditional way, by using @a("grid (or regular) sampling", href:= anchor(gridSampling)),or by @a("sampling uniformly", href:= "#UniformDistributionSampling") inside their domain.
@br
For higher dimension input space, specific statistics techniques ensuring low discrepency like @a("Latin Hypercube Sampling and SobolSequence", href := "#LatinHypercubeSobolSequence") are available.
@br
If you want to use design of experiments of your own you may also want to provide @a("a csv file with your samples" , href := "#CSVSampling") to OpenMOLE.

@br
By defining your own exploration task on @a("several types of input", href := anchor(severalInputs)), you will be able to highlight some of your model inner properties like those revealed by @a("sensitivity analysis", href:=Link.intern("Sensitivity analysis")), as shown in a toy example on a @a("real world example", href:=Link.intern("Real world Example"))


@h2{@gridSampling}

For a reasonable number of dimension and discretisation quanta (steps) values, complete sampling (or grid sampling)  consists of producing every combination of
the inputs possibles values, given their bounds and quanta of discretisation.
@br
@br
@Resource.rawFrag(Resource.img.method.completeID)
@br
@b{Method scores}
@br
Regular sampling or Uniform Sampling are quite good for a first Input Space exploration when you don't know anything  about its structure yet.
Since it samples from the input space, the collected values from the model executions will reveal the output values
obtained for "evenly spaced" inputs.
Sure it's not perfect, but still , it gives a little bit of insight about model sensitivity (as input values vary
within their domain) and if the output are fitness, it may present a little bit of optimization information (as the zone in which the fitness could be
minimized).
@br The sampling does not reveal anything about the output space structure, as there is no reason than evenly spaced inputs lead
to evenly spaced outputs.
Grid sampling is hampered by input space dimensionality as high dimension spaces need a lot of samples to be covered, as well as a lot of memory to store them.

@break
Grid Sampling is declared via a @b{DirectSampling Task}, where the bounds and discretisation quantum of each input to vary  are declared  for each input

@break
@hl.openmole("""
   val input_i = Val[Int]
   val input_j = Val[Double]

   DirectSampling(
     evaluation = my_own_evaluation  ,
     sampling =
       (input_i in (0 to 10 by 2)) x
       (input_j in (0.0 to 5.0 by 0.5)),
     aggregation= my_aggregation
   )""", name = "syntax of DirectSampling Task", header = "val my_own_evaluation = EmptyTask(); val my_aggregation = EmptyTask()")

@br
with 
    @ul
    @li{@b{@hl.code(" evaluation")} is the task (or a composition of tasks) that uses your inputs, typically your model task and a hook.}
    @li{@b{@hl.code("sampling")} is the sampling task}
    @li{@b{@hl.code("aggregation")} (@i{optional}) is an aggregation task to be performed on the outputs of your evaluation task}

@break
    Let's see it in action within a dummy workflow; Suppose we want to explore a model written in java, taking an integer value as input, and generating a String as output.

The exploration script would look like:

@hl.openmole("""
//inputs and outputs declaration
val i = Val[Int]
val o = Val[Double]
val avg = Val[Double]

//Defines the "model" task
val myModel =
  ScalaTask("val o = i * 2") set (
    inputs += i,
    outputs += (i, o)
  )

val average =
  ScalaTask("val avg = o.average") set (
    inputs += o.toArray,
    outputs += avg
  )

DirectSampling(
  evaluation = myModel hook DisplayHook(),
  sampling = i in (0 to 10 by 1),
  aggregation = average hook DisplayHook()
)""", name="concrete example of direct sampling")

Some details:
@ul
 @li{@hl.code("myModel") is the task that multiply the input by 2}
 @li{the @hl.code("evaluation") attribute of the @hl.code("DirectSampling") task is the composition of myModel and a hook}
 @li{the @hl.code("aggregation") attribute of the @hl.code("DirectSampling") task is the @hl.code("average") task, a ScalaTask that compute the average of an array Double values}
 @li{the task declared under the name @hl.code("exploration") is a DirectSampling task, which means it will generate parallel executions of myModel, one for each sample generated by the sampling task}

@break


@b{DirectSampling} generates a workflow that is illustrated below. You may recognize the @i{map reduce} design pattern, provided that an aggregation operator is defined (otherwise it would just be a @i{map} :-) )

@br

@img(src := Resource.img.method.directSampling.file, center(50))


@break

@h2{Model replication}

In the case of a stochastic model, you may want to define a replication task to run several replications of the model for the same parameter values.
This is similar to using a uniform distribution sampling on the seed of the model, and OpenMOLE provides a specific constructor for that, namely @code{Replication}.

The use of a @code{Replication} sampling is the following:

@hl.openmole("""
val mySeed = Val[Int]
val i = Val[Int]
val o = Val[Double]

val myModel =
  ScalaTask("import scala.util.Random; val rng = new Random(mySeed); val o = i * 2 + 0.1 * rng.nextDouble()") set (
    inputs += (i, mySeed),
    outputs += (i, o)
  )

Replication(
  evaluation = myModel,
  seed = mySeed,
  replications = 100
)""", name="example of replication", header = "val myModel = EmptyTask()")

The arguments for @code{Replication} are the following:

@ul
    @li{@b{@hl.code("evaluation")} is the task (or a composition of tasks) that uses your inputs, typically your model task and a hook.}
    @li{@b{@hl.code("seed")} is the prototype for the seed, which will be sampled with an uniform distribution in its domain (Val[Int] or Val[Long]).}
    @li{@b{@hl.code("replications")} (Int) is the number of replications.}
    @li{@b{@hl.code("distributionSeed")} (@i{optional}, Long) is an optional seed to be given to he uniform distribution of the seed ("meta-seed").}
    @li{@b{@hl.code("aggregation")} (@i{optional}) is an aggregation task to be performed on the outputs of your evaluation task.}


@break


@h2{@severalInputs}


Sampling can be performed on several inputs domains as well as on @b{several input types}, using the @b{cartesian product} operator: @b{x}, introduced in the @a("grid sampling",href:=anchor(gridSampling))  dedicated section.

Here is an example, still supposing you have already defined a task used for evaluation called myModel:


@hl.openmole("""
 val i = Val[Int]
 val j = Val[Double]
 val k = Val[String]
 val l = Val[Long]
 val m = Val[File]

 val exploration =
   DirectSampling (
   evaluation = myModel,
   sampling =
     (i in (0 to 10 by 2)) x
     (j in (0.0 to 5.0 by 0.5)) x
     (k in List("Leonardo", "Donatello", "RaphaÃ«l", "Michelangelo")) x
     (l in (UniformDistribution[Long]() take 10)) x
     (m in (workDirectory / "dir").files().filter(f => f.getName.startsWith("exp") && f.getName.endsWith(".csv")))
   )
  """, name = "several inputs", header = "val myModel = EmptyTask()")

@br
DirectSampling performs every combination between the 5 inputs of various types: Integer (i) , Double (j), Strings (k), Long (l), Files (m).

@br
@br
The UniformDistribution[Long]() take 10 is a uniform sampling of 10 numbers of the Long type, taken in the [Long.MIN_VALUE; Long.MAX_VALUE] domain of the Long native type.

@br
@br

Files are explored as items of a list.
 The items are gathered  by the @hl.code("files()") function applied  on the @hl.highlight("dir","plain") directory,
 optionally filtered with any @hl.code("String => Boolean")  functions  such as  @hl.highlight("contains(), startswith(), endswith()", "plain")


 (see  the @a("Java Class String Documentation", href:= shared.link.javaString)
 for more details)


@br
    If your input is one file among many,  or  a line among a CSV file, use the
    @a("CSVSampling task", href := DocumentationPages.csvSampling.file)
    and @a("FileSampling task", href := fileSampling.file + anchor("Exploring a set of files")).



